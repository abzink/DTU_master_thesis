{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from semi_supervised_transformers import PreprocessingPipeline\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n9/qhmmsj1s7w52648j4cjf7ztc0000gn/T/ipykernel_75839/639417878.py:1: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('/Users/annabzinkowska/DTU/master_thesis/data/processed_data_all_bertopic.csv')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/annabzinkowska/DTU/master_thesis/data/processed_data_all_bertopic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>description</th>\n",
       "      <th>month_mode</th>\n",
       "      <th>quarter_mode</th>\n",
       "      <th>year_mode</th>\n",
       "      <th>day_week_mode</th>\n",
       "      <th>quantity_sum</th>\n",
       "      <th>price_sum</th>\n",
       "      <th>unit_weight</th>\n",
       "      <th>customer_country_mode</th>\n",
       "      <th>customer_country_count</th>\n",
       "      <th>customer_id_count</th>\n",
       "      <th>category</th>\n",
       "      <th>unit_price_mean</th>\n",
       "      <th>description_original</th>\n",
       "      <th>topic</th>\n",
       "      <th>category_bertopic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>010004</td>\n",
       "      <td>cfcf sfy bolted hanger nptf stud viton</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>232.92</td>\n",
       "      <td>0.124</td>\n",
       "      <td>DK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>38.8200</td>\n",
       "      <td>C24FCF-1/2-SFY-S              BOLTED HANGER, N...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010005</td>\n",
       "      <td>cfcf sfy bolted hanger nptf stud viton</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>1774.00</td>\n",
       "      <td>0.259</td>\n",
       "      <td>CO</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>8.8700</td>\n",
       "      <td>C24FCF-1-SFY-S                BOLTED HANGER, N...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>010006</td>\n",
       "      <td>cfcf sfy bolted hanger nptf stud viton</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>1848.00</td>\n",
       "      <td>0.276</td>\n",
       "      <td>CO</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>6.1600</td>\n",
       "      <td>C24FCF-1 1/2-SFY-S            BOLTED HANGER, N...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>010015</td>\n",
       "      <td>cfcf sfy bolted hanger nptf stud viton</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>2115.00</td>\n",
       "      <td>0.318</td>\n",
       "      <td>SE</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>C24FCF-2-SFY-S                BOLTED HANGER, N...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>010040</td>\n",
       "      <td>cfcf sfy bolted hanger nptf stud viton</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>400</td>\n",
       "      <td>3119.00</td>\n",
       "      <td>0.363</td>\n",
       "      <td>CO</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>7.7975</td>\n",
       "      <td>C24FCF-3-SFY-S                BOLTED HANGER, N...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product_id                             description  month_mode  \\\n",
       "0     010004  cfcf sfy bolted hanger nptf stud viton           2   \n",
       "1     010005  cfcf sfy bolted hanger nptf stud viton          10   \n",
       "2     010006  cfcf sfy bolted hanger nptf stud viton          10   \n",
       "3     010015  cfcf sfy bolted hanger nptf stud viton           1   \n",
       "4     010040  cfcf sfy bolted hanger nptf stud viton           1   \n",
       "\n",
       "   quarter_mode  year_mode  day_week_mode  quantity_sum  price_sum  \\\n",
       "0             1       2020              2             6     232.92   \n",
       "1             4       2019              2           200    1774.00   \n",
       "2             4       2019              2           300    1848.00   \n",
       "3             1       2019              2           300    2115.00   \n",
       "4             1       2019              2           400    3119.00   \n",
       "\n",
       "   unit_weight customer_country_mode  customer_country_count  \\\n",
       "0        0.124                    DK                       1   \n",
       "1        0.259                    CO                       1   \n",
       "2        0.276                    CO                       1   \n",
       "3        0.318                    SE                       2   \n",
       "4        0.363                    CO                       2   \n",
       "\n",
       "   customer_id_count category  unit_price_mean  \\\n",
       "0                  1       -1          38.8200   \n",
       "1                  1       -1           8.8700   \n",
       "2                  1       -1           6.1600   \n",
       "3                  2       -1           7.0500   \n",
       "4                  2       -1           7.7975   \n",
       "\n",
       "                                description_original  topic category_bertopic  \n",
       "0  C24FCF-1/2-SFY-S              BOLTED HANGER, N...     -1                -1  \n",
       "1  C24FCF-1-SFY-S                BOLTED HANGER, N...     -1                -1  \n",
       "2  C24FCF-1 1/2-SFY-S            BOLTED HANGER, N...     -1                -1  \n",
       "3  C24FCF-2-SFY-S                BOLTED HANGER, N...     -1                -1  \n",
       "4  C24FCF-3-SFY-S                BOLTED HANGER, N...     -1                -1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['month_mode', 'quarter_mode','year_mode','day_week_mode', 'customer_country_mode']\n",
    "numerical_columns = ['quantity_sum', 'price_sum', 'unit_weight', 'unit_price_mean', 'customer_country_count', 'customer_id_count'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will train each classifier using the self-training approach, evaluate it on the validation set, and then display a ranking of classifiers by accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_bertopic\n",
       "-1    16969\n",
       "C     11939\n",
       "B      3513\n",
       "A      2505\n",
       "E       154\n",
       "D       106\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['category_bertopic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train: 14573\n",
      "Length of y_train: 14573\n",
      "Length of X_val: 3644\n",
      "Length of y_val: 3644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annabzinkowska/anaconda3/envs/thesis_/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train_processed: 14573\n",
      "Length of X_val_processed: 3644\n",
      "Length of X_unlabeled_processed: 16969\n"
     ]
    }
   ],
   "source": [
    "# Assuming `data` is your DataFrame and `categorical_columns`, `numerical_columns`, \n",
    "# and `log_transform_columns` are lists of column names\n",
    "\n",
    "# Set a random seed for replicability\n",
    "random_seed = 42\n",
    "\n",
    "# Splitting data into labeled and unlabeled samples\n",
    "labeled_data = data[data['category_bertopic'] != '-1']\n",
    "unlabeled_data = data[data['category_bertopic'] == '-1']\n",
    "\n",
    "X_labeled = labeled_data.drop(columns='category_bertopic')\n",
    "y_labeled = labeled_data['category_bertopic']\n",
    "X_unlabeled = unlabeled_data.drop(columns='category_bertopic')\n",
    "\n",
    "# Further split labeled data for evaluation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_labeled, y_labeled, test_size=0.2, stratify=y_labeled, random_state=random_seed)\n",
    "\n",
    "# Check lengths\n",
    "print(\"Length of X_train:\", len(X_train))\n",
    "print(\"Length of y_train:\", len(y_train))\n",
    "print(\"Length of X_val:\", len(X_val))\n",
    "print(\"Length of y_val:\", len(y_val))\n",
    "\n",
    "# Preprocess the data\n",
    "pipeline = PreprocessingPipeline(categorical_columns, numerical_columns, text_column='description')\n",
    "X_train_processed = pipeline.fit_transform(X_train, include_text=True)\n",
    "X_val_processed = pipeline.transform(X_val, include_text=True)\n",
    "X_unlabeled_processed = pipeline.transform(X_unlabeled, include_text=True)\n",
    "\n",
    "# Double-check lengths after processing\n",
    "print(\"Length of X_train_processed:\", len(X_train_processed))\n",
    "print(\"Length of X_val_processed:\", len(X_val_processed))\n",
    "print(\"Length of X_unlabeled_processed:\", len(X_unlabeled_processed))\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = [\n",
    "   ('Random Forest', RandomForestClassifier(random_state=random_seed)),\n",
    "   ('XGBoost', xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=random_seed)),\n",
    "   ('LightGBM', lgb.LGBMClassifier(random_state=random_seed)),\n",
    "   ('SVM', SVC(probability=True, random_state=random_seed)),\n",
    "   ('KNN', KNeighborsClassifier()),\n",
    "   ('Neural Network (MLP)', MLPClassifier(max_iter=1000, random_state=random_seed)),\n",
    "]\n",
    "\n",
    "# Fit the LabelEncoder on labels from the labeled dataset\n",
    "le = LabelEncoder()\n",
    "le.fit(y_labeled)\n",
    "\n",
    "# Encode the labeled data\n",
    "y_train_encoded = le.transform(y_train)\n",
    "y_val_encoded = le.transform(y_val)\n",
    "\n",
    "# Define a placeholder value for unlabeled data\n",
    "# We'll use the next integer after the maximum label for labeled data\n",
    "unlabeled_placeholder = len(le.classes_)\n",
    "\n",
    "# Confidence threshold for self-training, default 0.75\n",
    "threshold = 0.80\n",
    "\n",
    "# Track the number of samples pseudo-labeled in each iteration\n",
    "pseudo_labeled_counts = []\n",
    "\n",
    "# Training and evaluation\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Random Forest:\n",
      "Accuracy: 0.9758507135016465\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.99      0.99      0.99       501\n",
      "           B       0.98      0.94      0.96       703\n",
      "           C       0.97      1.00      0.98      2388\n",
      "           D       1.00      0.48      0.65        21\n",
      "           E       1.00      0.26      0.41        31\n",
      "\n",
      "    accuracy                           0.98      3644\n",
      "   macro avg       0.99      0.73      0.80      3644\n",
      "weighted avg       0.98      0.98      0.97      3644\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 497    1    3    0    0]\n",
      " [   1  660   42    0    0]\n",
      " [   1    6 2381    0    0]\n",
      " [   0    1   10   10    0]\n",
      " [   3    3   17    0    8]]\n",
      "Pseudo-labeled Count: 14973\n",
      "--------------------------------------------------\n",
      "Results for XGBoost:\n",
      "Accuracy: 0.9777716794731065\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.99      0.99      0.99       501\n",
      "           B       0.99      0.93      0.96       703\n",
      "           C       0.97      1.00      0.99      2388\n",
      "           D       0.94      0.71      0.81        21\n",
      "           E       1.00      0.32      0.49        31\n",
      "\n",
      "    accuracy                           0.98      3644\n",
      "   macro avg       0.98      0.79      0.85      3644\n",
      "weighted avg       0.98      0.98      0.98      3644\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 497    2    2    0    0]\n",
      " [   2  657   44    0    0]\n",
      " [   3    0 2384    1    0]\n",
      " [   0    2    4   15    0]\n",
      " [   2    3   16    0   10]]\n",
      "Pseudo-labeled Count: 16775\n",
      "--------------------------------------------------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008592 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9983\n",
      "[LightGBM] [Info] Number of data points in the train set: 14573, number of used features: 177\n",
      "[LightGBM] [Info] Start training from score -1.984025\n",
      "[LightGBM] [Info] Start training from score -1.645986\n",
      "[LightGBM] [Info] Start training from score -0.422525\n",
      "[LightGBM] [Info] Start training from score -5.144275\n",
      "[LightGBM] [Info] Start training from score -4.774741\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13120\n",
      "[LightGBM] [Info] Number of data points in the train set: 29998, number of used features: 188\n",
      "[LightGBM] [Info] Start training from score -2.669725\n",
      "[LightGBM] [Info] Start training from score -1.609538\n",
      "[LightGBM] [Info] Start training from score -0.325433\n",
      "[LightGBM] [Info] Start training from score -5.626755\n",
      "[LightGBM] [Info] Start training from score -5.311674\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13348\n",
      "[LightGBM] [Info] Number of data points in the train set: 31025, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score -2.691904\n",
      "[LightGBM] [Info] Start training from score -1.599655\n",
      "[LightGBM] [Info] Start training from score -0.326090\n",
      "[LightGBM] [Info] Start training from score -5.624050\n",
      "[LightGBM] [Info] Start training from score -5.312111\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009609 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13452\n",
      "[LightGBM] [Info] Number of data points in the train set: 31238, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score -2.693053\n",
      "[LightGBM] [Info] Start training from score -1.597916\n",
      "[LightGBM] [Info] Start training from score -0.326433\n",
      "[LightGBM] [Info] Start training from score -5.630892\n",
      "[LightGBM] [Info] Start training from score -5.312438\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012398 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13460\n",
      "[LightGBM] [Info] Number of data points in the train set: 31302, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score -2.694627\n",
      "[LightGBM] [Info] Start training from score -1.596330\n",
      "[LightGBM] [Info] Start training from score -0.326795\n",
      "[LightGBM] [Info] Start training from score -5.624049\n",
      "[LightGBM] [Info] Start training from score -5.308012\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13464\n",
      "[LightGBM] [Info] Number of data points in the train set: 31334, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score -2.695176\n",
      "[LightGBM] [Info] Start training from score -1.596564\n",
      "[LightGBM] [Info] Start training from score -0.326710\n",
      "[LightGBM] [Info] Start training from score -5.625071\n",
      "[LightGBM] [Info] Start training from score -5.302603\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13467\n",
      "[LightGBM] [Info] Number of data points in the train set: 31358, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score -2.695469\n",
      "[LightGBM] [Info] Start training from score -1.596700\n",
      "[LightGBM] [Info] Start training from score -0.326635\n",
      "[LightGBM] [Info] Start training from score -5.625837\n",
      "[LightGBM] [Info] Start training from score -5.303369\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013088 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13469\n",
      "[LightGBM] [Info] Number of data points in the train set: 31383, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score -2.696266\n",
      "[LightGBM] [Info] Start training from score -1.596710\n",
      "[LightGBM] [Info] Start training from score -0.326636\n",
      "[LightGBM] [Info] Start training from score -5.626634\n",
      "[LightGBM] [Info] Start training from score -5.291427\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011584 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13470\n",
      "[LightGBM] [Info] Number of data points in the train set: 31393, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score -2.696585\n",
      "[LightGBM] [Info] Start training from score -1.596871\n",
      "[LightGBM] [Info] Start training from score -0.326558\n",
      "[LightGBM] [Info] Start training from score -5.626952\n",
      "[LightGBM] [Info] Start training from score -5.291745\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13470\n",
      "[LightGBM] [Info] Number of data points in the train set: 31404, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score -2.696935\n",
      "[LightGBM] [Info] Start training from score -1.597064\n",
      "[LightGBM] [Info] Start training from score -0.326466\n",
      "[LightGBM] [Info] Start training from score -5.627303\n",
      "[LightGBM] [Info] Start training from score -5.292096\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13470\n",
      "[LightGBM] [Info] Number of data points in the train set: 31414, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score -2.697254\n",
      "[LightGBM] [Info] Start training from score -1.597068\n",
      "[LightGBM] [Info] Start training from score -0.326432\n",
      "[LightGBM] [Info] Start training from score -5.627621\n",
      "[LightGBM] [Info] Start training from score -5.292414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012494 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13471\n",
      "[LightGBM] [Info] Number of data points in the train set: 31402, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score -2.696872\n",
      "[LightGBM] [Info] Start training from score -1.600308\n",
      "[LightGBM] [Info] Start training from score -0.325564\n",
      "[LightGBM] [Info] Start training from score -5.627239\n",
      "[LightGBM] [Info] Start training from score -5.292032\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13473\n",
      "[LightGBM] [Info] Number of data points in the train set: 31418, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score -2.697381\n",
      "[LightGBM] [Info] Start training from score -1.600660\n",
      "[LightGBM] [Info] Start training from score -0.325413\n",
      "[LightGBM] [Info] Start training from score -5.627748\n",
      "[LightGBM] [Info] Start training from score -5.292541\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13472\n",
      "[LightGBM] [Info] Number of data points in the train set: 31431, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score -2.697322\n",
      "[LightGBM] [Info] Start training from score -1.601074\n",
      "[LightGBM] [Info] Start training from score -0.325298\n",
      "[LightGBM] [Info] Start training from score -5.628162\n",
      "[LightGBM] [Info] Start training from score -5.292955\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012541 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13474\n",
      "[LightGBM] [Info] Number of data points in the train set: 31439, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score -2.697577\n",
      "[LightGBM] [Info] Start training from score -1.601328\n",
      "[LightGBM] [Info] Start training from score -0.325200\n",
      "[LightGBM] [Info] Start training from score -5.628417\n",
      "[LightGBM] [Info] Start training from score -5.293209\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13474\n",
      "[LightGBM] [Info] Number of data points in the train set: 31445, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score -2.697768\n",
      "[LightGBM] [Info] Start training from score -1.601519\n",
      "[LightGBM] [Info] Start training from score -0.325170\n",
      "[LightGBM] [Info] Start training from score -5.628607\n",
      "[LightGBM] [Info] Start training from score -5.287091\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013471 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13474\n",
      "[LightGBM] [Info] Number of data points in the train set: 31448, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score -2.697863\n",
      "[LightGBM] [Info] Start training from score -1.601457\n",
      "[LightGBM] [Info] Start training from score -0.325178\n",
      "[LightGBM] [Info] Start training from score -5.628703\n",
      "[LightGBM] [Info] Start training from score -5.287186\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011063 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13474\n",
      "[LightGBM] [Info] Number of data points in the train set: 31451, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score -2.697959\n",
      "[LightGBM] [Info] Start training from score -1.601552\n",
      "[LightGBM] [Info] Start training from score -0.325141\n",
      "[LightGBM] [Info] Start training from score -5.628798\n",
      "[LightGBM] [Info] Start training from score -5.287282\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13474\n",
      "[LightGBM] [Info] Number of data points in the train set: 31453, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score -2.698022\n",
      "[LightGBM] [Info] Start training from score -1.601458\n",
      "[LightGBM] [Info] Start training from score -0.325161\n",
      "[LightGBM] [Info] Start training from score -5.628862\n",
      "[LightGBM] [Info] Start training from score -5.287345\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13474\n",
      "[LightGBM] [Info] Number of data points in the train set: 31453, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score -2.698022\n",
      "[LightGBM] [Info] Start training from score -1.601458\n",
      "[LightGBM] [Info] Start training from score -0.325161\n",
      "[LightGBM] [Info] Start training from score -5.628862\n",
      "[LightGBM] [Info] Start training from score -5.287345\n",
      "Results for LightGBM:\n",
      "Accuracy: 0.9799670691547749\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.99      0.99      0.99       501\n",
      "           B       0.98      0.94      0.96       703\n",
      "           C       0.98      1.00      0.99      2388\n",
      "           D       0.90      0.86      0.88        21\n",
      "           E       0.92      0.39      0.55        31\n",
      "\n",
      "    accuracy                           0.98      3644\n",
      "   macro avg       0.96      0.83      0.87      3644\n",
      "weighted avg       0.98      0.98      0.98      3644\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 496    2    2    0    1]\n",
      " [   2  663   38    0    0]\n",
      " [   1    3 2382    2    0]\n",
      " [   0    1    2   18    0]\n",
      " [   1    5   13    0   12]]\n",
      "Pseudo-labeled Count: 16829\n",
      "--------------------------------------------------\n",
      "Results for SVM:\n",
      "Accuracy: 0.9495060373216246\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.97      0.98      0.98       501\n",
      "           B       0.92      0.89      0.90       703\n",
      "           C       0.95      0.98      0.96      2388\n",
      "           D       1.00      0.33      0.50        21\n",
      "           E       1.00      0.26      0.41        31\n",
      "\n",
      "    accuracy                           0.95      3644\n",
      "   macro avg       0.97      0.69      0.75      3644\n",
      "weighted avg       0.95      0.95      0.95      3644\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 491    0   10    0    0]\n",
      " [   6  623   74    0    0]\n",
      " [   6   51 2331    0    0]\n",
      " [   0    3   11    7    0]\n",
      " [   3    2   18    0    8]]\n",
      "Pseudo-labeled Count: 16438\n",
      "--------------------------------------------------\n",
      "Results for KNN:\n",
      "Accuracy: 0.7889681668496158\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.74      0.59      0.66       501\n",
      "           B       0.71      0.49      0.58       703\n",
      "           C       0.81      0.93      0.87      2388\n",
      "           D       0.00      0.00      0.00        21\n",
      "           E       0.90      0.29      0.44        31\n",
      "\n",
      "    accuracy                           0.79      3644\n",
      "   macro avg       0.63      0.46      0.51      3644\n",
      "weighted avg       0.78      0.79      0.77      3644\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 297   34  170    0    0]\n",
      " [  35  344  324    0    0]\n",
      " [  57  105 2225    0    1]\n",
      " [   2    2   17    0    0]\n",
      " [  10    1   11    0    9]]\n",
      "Pseudo-labeled Count: 11444\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annabzinkowska/anaconda3/envs/thesis_/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/annabzinkowska/anaconda3/envs/thesis_/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/annabzinkowska/anaconda3/envs/thesis_/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/annabzinkowska/anaconda3/envs/thesis_/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "for name, base_classifier in classifiers:\n",
    "    if hasattr(base_classifier, 'predict_proba'):\n",
    "        self_training_model = SelfTrainingClassifier(base_classifier, criterion='threshold', threshold=threshold)\n",
    "\n",
    "        # Combine the data\n",
    "        combined_X = np.vstack((X_train_processed, X_unlabeled_processed))\n",
    "        combined_y = np.concatenate([y_train_encoded, [-1] * len(X_unlabeled_processed)])\n",
    "\n",
    "        # Track the number of samples pseudo-labeled in this iteration\n",
    "        pseudo_labels = np.full(len(X_unlabeled_processed), -1)  # Placeholder labels for unlabeled data\n",
    "        self_training_model.fit(combined_X, combined_y)\n",
    "\n",
    "        # Get the model's confidence scores for the unlabeled data\n",
    "        confidence_scores = self_training_model.predict_proba(X_unlabeled_processed)\n",
    "\n",
    "        # Check which samples meet the confidence threshold for pseudo-labeling\n",
    "        confident_samples = np.max(confidence_scores, axis=1) >= threshold\n",
    "\n",
    "        # Update the pseudo-labels for confident samples\n",
    "        pseudo_labels[confident_samples] = self_training_model.classes_[np.argmax(confidence_scores, axis=1)][confident_samples]\n",
    "\n",
    "        # Count the number of pseudo-labeled samples in this iteration\n",
    "        pseudo_labeled_count = np.sum(pseudo_labels != -1)\n",
    "        pseudo_labeled_counts.append((name, pseudo_labeled_count))\n",
    "\n",
    "        # Update the combined_y with pseudo-labels\n",
    "        combined_y[len(y_train_encoded):] = pseudo_labels\n",
    "\n",
    "        # Fit the model with updated pseudo-labels\n",
    "        self_training_model.fit(combined_X, combined_y)\n",
    "\n",
    "        # Evaluate the classifier after self-training\n",
    "        y_pred = self_training_model.predict(X_val_processed)\n",
    "        acc = accuracy_score(y_val_encoded, y_pred)\n",
    "\n",
    "        # Convert y_pred back to original labels for reporting\n",
    "        y_pred_original = le.inverse_transform(y_pred)\n",
    "        y_val_original = le.inverse_transform(y_val_encoded)\n",
    "\n",
    "        print(f\"Results for {name}:\")\n",
    "        print(\"Accuracy:\", acc)\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_val_original, y_pred_original))\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_val_original, y_pred_original))\n",
    "        print(\"Pseudo-labeled Count:\", pseudo_labeled_count)  # Print the number of pseudo-labeled samples\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        results.append((name, acc))\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping {name} as it does not support predict_proba\")\n",
    "\n",
    "# Sort and display classifiers by accuracy\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"Classifier rankings:\")\n",
    "for i, (name, acc) in enumerate(results):\n",
    "    print(f\"{i + 1}. {name}: {acc:.4f}\")\n",
    "\n",
    "# Print the number of pseudo-labeled samples for each classifier\n",
    "print(\"Pseudo-labeled Counts:\")\n",
    "for name, count in pseudo_labeled_counts:\n",
    "    print(f\"{name}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
